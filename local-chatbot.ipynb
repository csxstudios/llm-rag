{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "                                              0.0/817.7 kB ? eta -:--:--\n",
      "     -------------                          286.7/817.7 kB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------      716.8/817.7 kB 7.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 817.7/817.7 kB 7.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (0.2.56+cpuavx2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "     --------                                 0.5/2.1 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------                          0.8/2.1 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------                          0.8/2.1 MB 10.2 MB/s eta 0:00:01\n",
      "     -----------------------                  1.2/2.1 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------              1.4/2.1 MB 6.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      1.8/2.1 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (3.9.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "                                              0.0/1.9 MB ? eta -:--:--\n",
      "     ----------                               0.5/1.9 MB 15.5 MB/s eta 0:00:01\n",
      "     ------------------                       0.9/1.9 MB 11.0 MB/s eta 0:00:01\n",
      "     --------------------                     1.0/1.9 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------        1.6/1.9 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 7.7 MB/s eta 0:00:00\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "                                              0.0/291.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 291.3/291.3 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
      "                                              0.0/115.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 115.2/115.2 kB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from llama-cpp-python) (4.10.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from llama-cpp-python) (3.1.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "                                              0.0/49.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.4/49.4 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "                                              0.0/53.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.0/53.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "                                              0.0/292.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 292.8/292.8 kB 17.7 MB/s eta 0:00:00\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: packaging, mypy-extensions, jsonpointer, greenlet, typing-inspect, SQLAlchemy, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed SQLAlchemy-2.0.29 dataclasses-json-0.6.4 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 packaging-23.2 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\Chris\\Documents\\git\\llm-rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../../../.cache/lm-studio/models/monal04/llama-2-7b-chat.Q4_0.gguf-GGML/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '2', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized, ready to chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4151.43 ms\n",
      "llama_print_timings:      sample time =       3.31 ms /    13 runs   (    0.25 ms per token,  3933.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4151.39 ms /    16 tokens (  259.46 ms per token,     3.85 tokens per second)\n",
      "llama_print_timings:        eval time =    3187.96 ms /    12 runs   (  265.66 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:       total time =    7391.96 ms /    28 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who is the current president?', 'text': 'The current President of the United States is Joe Biden.'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4151.43 ms\n",
      "llama_print_timings:      sample time =       3.21 ms /    13 runs   (    0.25 ms per token,  4048.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3138.72 ms /    13 runs   (  241.44 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:       total time =    3186.58 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who is the current president?', 'text': 'The current president of the United States is Joe Biden.'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4151.43 ms\n",
      "llama_print_timings:      sample time =       3.37 ms /    16 runs   (    0.21 ms per token,  4742.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2040.06 ms /    11 tokens (  185.46 ms per token,     5.39 tokens per second)\n",
      "llama_print_timings:        eval time =    3528.29 ms /    15 runs   (  235.22 ms per token,     4.25 tokens per second)\n",
      "llama_print_timings:       total time =    5624.46 ms /    26 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the current date?', 'text': 'The current date is March 12, 2023.'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4151.43 ms\n",
      "llama_print_timings:      sample time =      59.66 ms /   256 runs   (    0.23 ms per token,  4291.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2810.35 ms /    13 tokens (  216.18 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:        eval time =   63045.86 ms /   255 runs   (  247.24 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:       total time =   66868.03 ms /   268 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is taggs.hhs.gov?', 'text': 'Taggs.hhs.gov is a website for the Healthcare IT (HIT) industry, specifically focused on government initiatives and regulations related to HIT. The website provides information and resources for healthcare providers, vendors, and other stakeholders interested in working with the US Department of Health and Human Services (HHS).\\n\\nThe site covers various topics such as:\\n\\n* Meaningful Use program - guidelines and incentives for adopting electronic health records (EHRs)\\n* Stage 3 requirements - guidelines for advanced EHR functionality\\n* Certified Health Information Technology (CHIT) - a list of certified EHRs and other HIT products\\n* Office of the National Coordinator for Health Information Technology (ONC) - news, events, and resources related to HIT policy and innovation\\n* Health Information Exchange (HIE) - information on exchanging patient data between different providers and organizations\\n* Public Health Information Network (PHIN) - resources for sharing public health data between states and territories\\n* Privacy and Security - guidelines and tools for protecting patient data privacy and security\\n* Personalized Medicine - resources for understanding'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load the LlamaCpp language model, adjust GPU usage based on your hardware\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../../../.cache/lm-studio/models/monal04/llama-2-7b-chat.Q4_0.gguf-GGML/llama-2-7b-chat.Q4_0.gguf\",\n",
    "    n_gpu_layers=40,\n",
    "    n_batch=512,  # Batch size for model processing\n",
    "    #verbose=False,  # Enable detailed logging for debugging\n",
    ")\n",
    "\n",
    "# Define the prompt template with a placeholder for the question\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Create an LLMChain to manage interactions with the prompt and model\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(\"Chatbot initialized, ready to chat...\")\n",
    "while True:\n",
    "    question = input(\"> \")\n",
    "    answer = llm_chain.invoke(question)\n",
    "    print(answer, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
